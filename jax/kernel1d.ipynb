{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax.numpy as jnp \n",
    "from jax import vmap, jit \n",
    "import scipy\n",
    "from scipy.special import roots_legendre, factorial\n",
    "from functools import partial\n",
    "\n",
    "from einops import repeat, rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    # ReLU activation function\n",
    "    # n : degree \n",
    "    return jnp.maximum(0,x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP1D(l,n):\n",
    "    x = np.linspace(0,1,n)\n",
    "    y = np.linspace(0,1,n)\n",
    "    mean = np.zeros_like(x)\n",
    "    gram = np.exp(-np.abs(x.reshape(-1,1) - y.reshape(1,-1))**2/(2*l**2))\n",
    "    f = np.random.multivariate_normal(mean, gram)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kernel(x, y):\n",
    "    return 0.5*(x+y-jnp.abs(x-y)) - x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 14:25:22.021529: W external/xla/xla/service/gpu/nvptx_compiler.cc:765] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "nSample = 20\n",
    "nPts = 513\n",
    "x = jnp.linspace(0,1,nPts)\n",
    "h = x[1]\n",
    "\n",
    "# eval kernel function\n",
    "xx, yy = jnp.meshgrid(x,x)\n",
    "xx, yy = xx.reshape(-1), yy.reshape(-1)\n",
    "K = Kernel(xx,yy).reshape(nPts, nPts)\n",
    "\n",
    "# eval force functions\n",
    "fs = []\n",
    "for i in range(nSample):\n",
    "    fs.append(GP1D(0.02,nPts))\n",
    "fs = jnp.array(fs) # nSample x npts\n",
    "\n",
    "# calc solution functions\n",
    "us = h* (K @ fs.T).T # nSample x npts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gforward(wx, wy, b):\n",
    "    return ReLU(wx*xs+wy*ys+b).reshape(nPts, nPts)\n",
    "\n",
    "gsforward = jit(vmap(gforward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_search(wxs, wys, bs, Gk):\n",
    "    gs = gsforward(wxs, wys, bs) # nParam x npts x npts\n",
    "    uhss = h * rearrange(gs @ fs.T, 'b n s -> b s n') # nParam x nSample x nPts \n",
    "    uts = h * rearrange(Gk @ fs.T, 'n s -> s n') # nSample x nPts\n",
    "    rG = h * ((us - uts) * uhss).sum(axis=(1,2))\n",
    "    E = -0.5 * rG ** 2\n",
    "    idx = jnp.argmin(E)\n",
    "    wxk, wyk, bk = wxs[idx], wys[idx], bs[idx]\n",
    "    return wxk, wyk, bk, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th - 2.9127e+03\n",
      "1.0 1.0 2.0\n",
      "-0.3547313\n",
      "[[88.46504]]\n",
      "[0.84229803]\n",
      "1th - 1.8835e+03\n",
      "1.0 1.0 2.0\n",
      "-0.1643003\n",
      "[[88.465034 88.465034]\n",
      " [88.465034 88.465034]]\n",
      "[0.84229803 0.84229803]\n",
      "2th - 1.8835e+03\n",
      "1.0 1.0 2.0\n",
      "-0.16430026\n",
      "[[88.46427 88.46427 88.46427]\n",
      " [88.46427 88.46427 88.46427]\n",
      " [88.46427 88.46427 88.46427]]\n",
      "[0.8422971 0.8422971 0.8422971]\n"
     ]
    }
   ],
   "source": [
    "nSample = 20\n",
    "nPts = 513\n",
    "x = jnp.linspace(0,1,nPts)\n",
    "xx, yy = jnp.meshgrid(x,x)\n",
    "xx, yy = xx.reshape(-1), yy.reshape(-1)\n",
    "\n",
    "nNeuron = 3\n",
    "xs = xx \n",
    "ys = yy \n",
    "h = 1/(nPts - 1)\n",
    "\n",
    "Gref = K \n",
    "OmegaX = []\n",
    "OmegaY = []\n",
    "Beta = []\n",
    "Alpha = []\n",
    "\n",
    "# learned kernel \n",
    "Gk = jnp.zeros((nPts, nPts))\n",
    "gbasis = jnp.zeros((nNeuron, nPts, nPts))\n",
    "\n",
    "# parameter space\n",
    "nw = 4\n",
    "nb = 1001 \n",
    "nParam = nw * nw * nb\n",
    "Wx = jnp.linspace(-1,1,nw)\n",
    "Wy = jnp.linspace(-1,1,nw)\n",
    "B = jnp.linspace(-2,2,nb)\n",
    "wwx, wwy, bb = jnp.meshgrid(Wx, Wy, B)\n",
    "wxs = wwx.reshape(-1)\n",
    "wys = wwy.reshape(-1)\n",
    "bs = bb.reshape(-1)\n",
    "\n",
    "for k in range(nNeuron):\n",
    "    # loss measure \n",
    "    l2 = ((Gk - Gref)**2).sum()\n",
    "    print('{:}th - {:.4e}'.format(k, l2))\n",
    "\n",
    "    # maximization step\n",
    "    wxk, wyk, bk, E = brute_search(wxs, wys, bs, Gk)\n",
    "    print(wxk, wyk, bk)\n",
    "    print(E.min())\n",
    "    # find new basis gk\n",
    "    gk = gforward(wxk, wyk, bk)\n",
    "    # print(gk)\n",
    "\n",
    "    # projection step\n",
    "    gbasis = gbasis.at[k].set(gk)\n",
    "    gsub = gbasis[:k+1] # k x nPts x nPts\n",
    "    uhsub = h * rearrange(gsub @ fs.T, 'k n s -> k s n')\n",
    "    A = h * jnp.einsum('kns,pns->kp', uhsub, uhsub)\n",
    "    b = h * (uhsub * us).sum(axis=(1,2))\n",
    "    print(A)\n",
    "    print(b)\n",
    "    alpha_k = jnp.linalg.solve(A, b).reshape(-1,1)\n",
    "\n",
    "    # update Gk \n",
    "    OmegaX.append(wxk)\n",
    "    OmegaY.append(wyk)\n",
    "    Beta.append(bk)\n",
    "\n",
    "    omegax_k = jnp.concatenate([x.reshape(1,1) for x in OmegaX])\n",
    "    omegay_k = jnp.concatenate([y.reshape(1,1) for y in OmegaY])\n",
    "    beta_k = jnp.concatenate([b.reshape(1,1) for b in Beta])\n",
    "\n",
    "    # print('alpha : ', alpha_k.shape)\n",
    "    # print('omegax : ', omegax_k.shape)\n",
    "    # print('omegay : ', omegay_k.shape)\n",
    "    # print('beta : ', beta_k.shape)\n",
    "    # print(((omegax_k * xs) + (omegay_k * ys) + beta_k).shape)\n",
    "    Gk = (alpha_k.T @ ((omegax_k * xs) + (omegay_k * ys) + beta_k)).reshape(nPts, nPts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.16430026, -0.16341607, -0.16254802, ..., -0.        ,\n",
       "       -0.        , -0.        ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
